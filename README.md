# RAG Query Pipeline Application

This repository provides a **Retrieval-Augmented Generation (RAG)** system that utilizes vector-based embeddings to retrieve relevant documents and query a Large Language Model (LLM) for contextually aware responses. The project is built using **FastAPI**, **Llama Index**, and other supporting libraries for seamless document indexing, retrieval, and querying.

## Features
- **Preprocessing Documents**: Converts raw data into semantic representations using embeddings.
- **Indexing and Retrieval**: Efficient storage and retrieval of documents using a vector-based index.
- **Query Processing**: Combines retrieved context and user queries to generate LLM responses.
- **Custom Embeddings**: Embedding generation is powered by an external API.
- **Web Interface**: Users can interact with the RAG system via a clean and responsive HTML-based UI.

## Project Structure
```
├── app.py                   # Main FastAPI application
├── src
│   ├── models
│   │   └── rag_utils.py    # Utilities for RAG pipeline
│   ├── templates           # HTML templates
│   │   ├── query_form.html
│   │   └── query_response.html
│   ├── static              # Static assets (CSS, JS)
│   │   └── styles.css
│   ├── utils
│   │   ├── data.py         # Contains models_list
│   │   ├── preprocess_data.py # Document preprocessing
│   │   └── generate_embedding.py  # Embedding generation logic
├── index_storage            # Directory for storing the vector index
├── .env                     # Environment variables
├── requirements.txt         # Python dependencies
└── README.md                # Project documentation
```

## Technologies Used
- **FastAPI**: For building the backend and serving the application.
- **Llama Index (GPT Vector Index)**: For creating , managing document indices and retriever for those Documents.
- **HTML & CSS**: For the user interface.
- **Requests**: To interact with the external embedding API.
- **Python Libraries**:
  - `dotenv`: For environment variable management.
  - `pydantic`: For data validation and structure.
  - `json`: For handling JSON-formatted data.
  - `os` and `pathlib`: For handling file and directory operations.
- **Nomic's `embed-text` Model**: Used for generating high-quality text embeddings.
- **Groq Llama 3-70B Model**: Powers the large language model (LLM) backend for generating context-aware responses.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo/rag-query-pipeline.git
   cd rag-query-pipeline
   ```

2. Create a virtual environment and install dependencies:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   pip install -r requirements.txt
   ```

3. Set up environment variables:
   - Create a `.env` file in the root directory.
   - Add your `groq_token` for LLM API authentication:
     ```env
     groq_token=<your_api_key>
     ```

4. Start the FastAPI application:
   ```bash
   uvicorn app:app --reload
   ```

5. Access the application in your browser at `http://127.0.0.1:8000`.

## Usage

### Web Interface
1. Open the application in your browser.
2. Enter a query in the form provided.
3. Submit the query to receive a context-aware response generated by the RAG pipeline.

### Programmatic Usage
The `rag_utils.py` module can be imported for programmatic access to the RAG pipeline:
```python
from src.models.rag_utils import rag_pipeline

response = rag_pipeline(index, llm, "What are the key features of this project?")
print(response)
```

## Key Components

### `app.py`
- Defines the FastAPI routes for the query form and response pages.
- Integrates the RAG pipeline for processing user queries.
- Serves static files and HTML templates.

### `rag_utils.py`
Contains utility functions:
- **`load_and_preprocess_documents`**: Prepares raw data for indexing.
- **`build_index`**: Creates a vector-based index for fast retrieval.
- **`save_index`**: Persists the index to disk.
- **`load_index`**: Loads a previously saved index.
- **`rag_pipeline`**: Orchestrates document retrieval and LLM query processing.

### `InstructorEmbeddings`
- Custom embedding generation class leveraging an external API.
- Supports generating embeddings for queries, single documents, and multiple documents.

### `Preprocessing`
- **`preprocess_documents`**: Converts raw data into `Document` objects used by the Llama Index.

## Environment Variables

- `groq_token`: API key for Groq LLM.

## Contribution
1. Fork the repository.
2. Create a new branch:
   ```bash
   git checkout -b feature-name
   ```
3. Make your changes and commit:
   ```bash
   git commit -m "Description of changes"
   ```
4. Push to your branch:
   ```bash
   git push origin feature-name
   ```
5. Submit a pull request.

## License
This project is licensed under the MIT License. See `LICENSE` for more information.

---

### Screenshots

**Query Form**
![Query Form Screenshot](src/static/images/query_form.png)

**Response Page**
![Response Page Screenshot](src/static/images/response_page.png)

